{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cec8a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython import display\n",
    "from tensorflow.keras import layers\n",
    "from time import strftime\n",
    "from scipy.signal import spectrogram, stft, istft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f937960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"GanPlayground\"\n",
    "SCALING_FACTOR = 0\n",
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 128\n",
    "NUM_EXAMPLES_TO_GENERATE = 1\n",
    "NOISE_DIM = 100\n",
    "STEAD_PATH_DB_PROCESSED_STFT = \"/Users/jarek/git/saigon/data/STEAD-processed-stft.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e329e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_stream(do, label, fs=100, nperseg=155, file_path=None):\n",
    "    d0 = pd.DataFrame(data=do)\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 16), dpi=60)\n",
    "    ax1 = plt.subplot2grid((4, 1), (0, 0))\n",
    "    ax2 = plt.subplot2grid((4, 1), (1, 0))\n",
    "    ax3 = plt.subplot2grid((4, 1), (2, 0), rowspan=2)\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    sns.lineplot(data=do, ax=ax1, linewidth=1, legend=None)\n",
    "\n",
    "    ax1.set_title(\"Waveform\")\n",
    "    ax1.set(xlabel=\"Samples\", ylabel=\"Amplitude counts\")\n",
    "    ax1.locator_params(nbins=6, axis=\"y\")\n",
    "\n",
    "    f, t, Sxx = spectrogram(x=do, fs=fs)\n",
    "\n",
    "    ax2.clear()\n",
    "    ax2.set_title(\"Spectrogram\")\n",
    "    ax2.pcolormesh(t, f, Sxx, shading=\"gouraud\")\n",
    "    ax2.set(xlabel=\"Time [sec]\", ylabel=\"Frequency [Hz]\")\n",
    "\n",
    "    f_sftt, t_sftt, Zxx = stft(do, window=\"hanning\", fs=fs, nperseg=nperseg)\n",
    "\n",
    "    ax3.clear()\n",
    "    ax3.set_title(\"STFT\")\n",
    "    ax3.pcolormesh(t_sftt, f_sftt, np.abs(Zxx), shading=\"auto\")\n",
    "\n",
    "    plt.suptitle(label, fontsize=14)\n",
    "\n",
    "    if file_path != None:\n",
    "        plt.savefig(file_path)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54931672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stft(stream, fs=100, nperseg=155):\n",
    "    f, t, Zxx = stft(stream, window='hanning', fs=fs, nperseg=nperseg)\n",
    "    # plt.specgram(x_1[0][0], cmap='plasma', Fs=100)\n",
    "    plt.pcolormesh(t, f, np.abs(Zxx), shading='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3cb57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stft_data(file_path, arr_length):\n",
    "    with h5py.File(file_path, \"r\") as f:\n",
    "        keys = f[\"keys\"][:arr_length]\n",
    "        components = f[\"components\"][:arr_length]\n",
    "        data = f[\"data\"][:arr_length]\n",
    "        return (keys, components, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f59a544",
   "metadata": {},
   "source": [
    "# Read processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee766d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(keys, components, x_train) = get_stft_data(\n",
    "    STEAD_PATH_DB_PROCESSED_STFT, 50000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997c16cf",
   "metadata": {},
   "source": [
    "# Convert streams to STFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdac3cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STFT of the stream and then reverse STFT back into original stream\n",
    "# f, t, Zxx = stft(x_1[1][0], window='hanning', fs=100, nperseg=155)\n",
    "# k2 = istft(Zxx, window='hanning', fs=100, nperseg=155)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe100827",
   "metadata": {},
   "source": [
    "# Scale and reshape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11812f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALING_FACTOR = int(\n",
    "    max(\n",
    "        [\n",
    "            abs(min([x.min() for x in x_train])),\n",
    "            abs(max([x.max() for x in x_train])),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "SCALING_FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5995606e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train /= SCALING_FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834b6812",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], 78, 78, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da77bff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(x_train)\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12895ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8761c2ef",
   "metadata": {},
   "source": [
    "# Logs and Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c7de6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = f\"{MODEL_NAME} at {strftime('%H:%M')}\"\n",
    "log_dir = os.path.join(\"../log/\", folder_name)\n",
    "\n",
    "try:\n",
    "    os.makedirs(log_dir)\n",
    "except OSError as exception:\n",
    "    print(exception.strerror)\n",
    "else:\n",
    "    print(\"Successfully created dirs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d1e592",
   "metadata": {},
   "source": [
    "# Define GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f75dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(3 * 3 * 128, use_bias=False, input_shape=(NOISE_DIM,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((3, 3, 128)))\n",
    "\n",
    "    model.add(\n",
    "        layers.Conv2DTranspose(\n",
    "            64, (20, 20), strides=(1, 1), padding=\"same\", use_bias=False\n",
    "        )\n",
    "    )\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(\n",
    "        layers.Conv2DTranspose(\n",
    "            64, (20, 20), strides=(2, 2), padding=\"same\", use_bias=False\n",
    "        )\n",
    "    )\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(\n",
    "        layers.Conv2DTranspose(\n",
    "            64, (20, 20), strides=(13, 13), padding=\"same\", use_bias=False\n",
    "        )\n",
    "    )\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(\n",
    "        layers.Conv2DTranspose(\n",
    "            1,\n",
    "            (10, 10),\n",
    "            strides=(1, 1),\n",
    "            padding=\"same\",\n",
    "            use_bias=False,\n",
    "            activation=\"tanh\",\n",
    "        )\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71ec9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "# noise = tf.random.normal(dtype=tf.dtypes.float32, shape=[78, 78], stddev=5)\n",
    "noise = tf.random.normal([BATCH_SIZE, NOISE_DIM], stddev=10e5)\n",
    "generated_stft = generator(noise, training=False)\n",
    "\n",
    "generated_stft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cfdcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2925e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(generator, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff74b1d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inversed = istft(generated_stft[0, :, :, 0], window='hanning', fs=100, nperseg=155)\n",
    "plot_single_stream(inversed[1][:6000], \"GAN Generator Noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a19e111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[78, 78, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc60f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_stft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df48ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c63242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c718af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75b26d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a855866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a02d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([NUM_EXAMPLES_TO_GENERATE, NOISE_DIM])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a0afba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e37b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      train_step(image_batch)\n",
    "\n",
    "    # Produce images for the GIF as you go\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  # Generate after the final epoch\n",
    "  display.clear_output(wait=True)\n",
    "  generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e83d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    # Notice `training` is set to False.\n",
    "    # This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training=False)\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        inversed = istft(\n",
    "            predictions[i, :, :, 0][:6000], window=\"hanning\", fs=100, nperseg=155\n",
    "        )\n",
    "        plot_single_stream(\n",
    "            inversed[1][:6000],\n",
    "            f\"GAN Event (epoch {epoch})\",\n",
    "            # file_path=f\"out/image_at_epoch_{epoch}.jpg\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6ff339",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebd24cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11fb87684b5e7e8b7a77280a5a30a28e41ff00f13a000705cb6714e5054607b4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
